{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analytics Associate, CrossInstall</td>\n",
       "      <td>General Mills</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATA SCIENTIST I</td>\n",
       "      <td>The Home Depot</td>\n",
       "      <td>Houston, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Scientist Apprenticeship</td>\n",
       "      <td>IBM</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science - Intern</td>\n",
       "      <td>Sonde Health, Inc.</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>Curology</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst - Chicago</td>\n",
       "      <td>SpiderRock</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Financial Analytics Consultant</td>\n",
       "      <td>Toyota North America</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Research Assistant IV Non-Lab (Research Data A...</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Analyst, Data and Analysis</td>\n",
       "      <td>Digitas India</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>CapTech Ventures, Inc</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Analyst Intern</td>\n",
       "      <td>Global Atlantic Financial Group</td>\n",
       "      <td>Des Moines, IA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Shoe Carnival, Inc.</td>\n",
       "      <td>Columbia, SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Quantitative Researcher</td>\n",
       "      <td>Pantera Capital</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fraud Intelligence, Data Operations Analyst</td>\n",
       "      <td>White Ops</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Science and Analytics</td>\n",
       "      <td>Thermo Fisher Scientific</td>\n",
       "      <td>Carlsbad, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UHNW Business Analytics</td>\n",
       "      <td>Mstream</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Analyst, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Junior Data Scientist - Model Governance</td>\n",
       "      <td>Cigna</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Aditi Consulting</td>\n",
       "      <td>Chandler, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>Koch Industries</td>\n",
       "      <td>Louisville, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Associate Analyst - Analytics</td>\n",
       "      <td>Barkley</td>\n",
       "      <td>Kansas City, KS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Massachusetts General Hospital</td>\n",
       "      <td>Charlestown, NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Datadog</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Lockheed Martin</td>\n",
       "      <td>Herndon, VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cortland</td>\n",
       "      <td>Atlanta Metropolitan Area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0              Data Analytics Associate, CrossInstall   \n",
       "1                                    DATA SCIENTIST I   \n",
       "2                Junior Data Scientist Apprenticeship   \n",
       "3                               Data Science - Intern   \n",
       "4                                 Data Science Intern   \n",
       "5                              Data Analyst - Chicago   \n",
       "6                      Financial Analytics Consultant   \n",
       "7   Research Assistant IV Non-Lab (Research Data A...   \n",
       "8                          Analyst, Data and Analysis   \n",
       "9                                        Data Analyst   \n",
       "10                                Data Analyst Intern   \n",
       "11                                       Data Analyst   \n",
       "12                            Quantitative Researcher   \n",
       "13        Fraud Intelligence, Data Operations Analyst   \n",
       "14                         Data Science and Analytics   \n",
       "15                            UHNW Business Analytics   \n",
       "16                         Analyst, Data and Analysis   \n",
       "17           Junior Data Scientist - Model Governance   \n",
       "18                                       Data Analyst   \n",
       "19                                     DATA SCIENTIST   \n",
       "20                      Associate Analyst - Analytics   \n",
       "21                                     Data Analyst I   \n",
       "22                                     Data Scientist   \n",
       "23                                     Data Scientist   \n",
       "24                                       Data Analyst   \n",
       "\n",
       "                            Company                         Location  \n",
       "0                     General Mills                San Francisco, CA  \n",
       "1                    The Home Depot                      Houston, TX  \n",
       "2                               IBM                     New York, NY  \n",
       "3                Sonde Health, Inc.                       Boston, MA  \n",
       "4                          Curology                San Francisco, CA  \n",
       "5                        SpiderRock                      Chicago, IL  \n",
       "6              Toyota North America                       Dallas, TX  \n",
       "7                Harvard University                       Boston, MA  \n",
       "8                     Digitas India                       Boston, MA  \n",
       "9             CapTech Ventures, Inc                    Charlotte, NC  \n",
       "10  Global Atlantic Financial Group                   Des Moines, IA  \n",
       "11              Shoe Carnival, Inc.                     Columbia, SC  \n",
       "12                  Pantera Capital                San Francisco, CA  \n",
       "13                        White Ops  New York City Metropolitan Area  \n",
       "14         Thermo Fisher Scientific                     Carlsbad, CA  \n",
       "15                          Mstream                     New York, NY  \n",
       "16            Digitas North America                       Boston, MA  \n",
       "17                            Cigna                     New York, NY  \n",
       "18                 Aditi Consulting                     Chandler, AZ  \n",
       "19                  Koch Industries                   Louisville, CO  \n",
       "20                          Barkley                  Kansas City, KS  \n",
       "21   Massachusetts General Hospital                  Charlestown, NH  \n",
       "22                          Datadog                     New York, NY  \n",
       "23                  Lockheed Martin                      Herndon, VA  \n",
       "24                         Cortland        Atlanta Metropolitan Area  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Analyst Level-1</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst Level-1</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Newark, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Stamford, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Analyst Fresher</td>\n",
       "      <td>Staffigo</td>\n",
       "      <td>Portland, OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Warehouse Architect</td>\n",
       "      <td>Kent Corporation</td>\n",
       "      <td>Davenport, IA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Data Analyst - Remote Position! - Dallas</td>\n",
       "      <td>Seasoned Recruitment</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Quantitative Trading Analyst Intern</td>\n",
       "      <td>DRW</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Massachusetts General Hospital</td>\n",
       "      <td>Charlestown, NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Booz Allen Hamilton</td>\n",
       "      <td>Arlington, VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Aditi Consulting</td>\n",
       "      <td>Chandler, AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title                         Company  \\\n",
       "0                    Business Analyst Level-1                        Staffigo   \n",
       "1                    Business Analyst Level-1                        Staffigo   \n",
       "2                            Business Analyst                        Staffigo   \n",
       "3                    Business Analyst Fresher                        Staffigo   \n",
       "4                    Data Warehouse Architect                Kent Corporation   \n",
       "..                                        ...                             ...   \n",
       "120  Data Analyst - Remote Position! - Dallas            Seasoned Recruitment   \n",
       "121       Quantitative Trading Analyst Intern                             DRW   \n",
       "122                            Data Analyst I  Massachusetts General Hospital   \n",
       "123                              Data Analyst             Booz Allen Hamilton   \n",
       "124                              Data Analyst                Aditi Consulting   \n",
       "\n",
       "            Location  \n",
       "0    Santa Clara, CA  \n",
       "1         Newark, NJ  \n",
       "2       Stamford, CT  \n",
       "3       Portland, OR  \n",
       "4      Davenport, IA  \n",
       "..               ...  \n",
       "120       Dallas, TX  \n",
       "121      Chicago, IL  \n",
       "122  Charlestown, NH  \n",
       "123    Arlington, VA  \n",
       "124     Chandler, AZ  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "def scrape_linkedin_job_search(keywords, num_page):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ([''.join([BASE_URL, 'keywords=', keywords, \"&start=\", str(25 *i)]) \n",
    "                  for i in range (num_page)])\n",
    "    soup = []\n",
    "    \n",
    "    for url in scrape_url:\n",
    "        if requests.get(url).status_code == 200:\n",
    "            soup.append(BeautifulSoup(requests.get(url).text, 'html.parser'))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Create a request to get the data from the server \n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for p in soup:\n",
    "        for card in p.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data\n",
    "\n",
    "results2 = scrape_linkedin_job_search('data%20analysis', 5)\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Junior) Data Scientist / Data Analyst (w/m/d)</td>\n",
       "      <td>KPMG Deutschland</td>\n",
       "      <td>Dresden, Saxony, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>datalytix.io</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Human Capital Advisory Group</td>\n",
       "      <td>Stuttgart Region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst (m/f/x)</td>\n",
       "      <td>Audible, Inc.</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>Weimar, Thuringia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Internship in Data Analytics</td>\n",
       "      <td>Henkel</td>\n",
       "      <td>Düsseldorf, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist / Data Analyst (m/f/d)</td>\n",
       "      <td>Chemovator GmbH</td>\n",
       "      <td>Mannheim, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Intern</td>\n",
       "      <td>Pandata</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst (f/m/x)</td>\n",
       "      <td>audibene</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Junior) Data Analyst - Marketing</td>\n",
       "      <td>Trade Republic</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Scientist (m|w|d)</td>\n",
       "      <td>Sartorius</td>\n",
       "      <td>Ulm, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst / Vertriebscontrolling / Data Sci...</td>\n",
       "      <td>Campusjäger</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>OneMagnify</td>\n",
       "      <td>Cologne, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data and Insights Analyst</td>\n",
       "      <td>TheSoul Publishing</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>Porsche Consulting Brasil</td>\n",
       "      <td>Tamm, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Product Analyst</td>\n",
       "      <td>AMBOSS</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Scientist - Cologne</td>\n",
       "      <td>Xcede</td>\n",
       "      <td>Cologne, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>hundred</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>datalytix.io</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Analyst, Data Management and Analysis (Remote)</td>\n",
       "      <td>Tent Partnership for Refugees</td>\n",
       "      <td>Frankfurt am Main, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Intern Opportunities for Students in Germany: ...</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Naturwissenschaftler - Data Analysis, Qualität...</td>\n",
       "      <td>BioSpring</td>\n",
       "      <td>Frankfurt, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Analyst</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Scientist (m/f/x)</td>\n",
       "      <td>CLARK</td>\n",
       "      <td>Frankfurt am Main, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analyst / Data Engineer (m/f/d)</td>\n",
       "      <td>uptodate Ventures GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0      (Junior) Data Scientist / Data Analyst (w/m/d)   \n",
       "1                                 Data Science Intern   \n",
       "2                                        Data Analyst   \n",
       "3                                Data Analyst (m/f/x)   \n",
       "4                              Data Scientist (m/f/d)   \n",
       "5                        Internship in Data Analytics   \n",
       "6               Data Scientist / Data Analyst (m/f/d)   \n",
       "7                                         Data Intern   \n",
       "8                                Data Analyst (f/m/x)   \n",
       "9                   (Junior) Data Analyst - Marketing   \n",
       "10                             Data Scientist (m|w|d)   \n",
       "11  Data Analyst / Vertriebscontrolling / Data Sci...   \n",
       "12                                       Data Analyst   \n",
       "13                         Data and Insights Analyst    \n",
       "14                             Data Scientist (m/f/d)   \n",
       "15                                    Product Analyst   \n",
       "16                           Data Scientist - Cologne   \n",
       "17                                     Data Scientist   \n",
       "18                                Data Science Intern   \n",
       "19     Analyst, Data Management and Analysis (Remote)   \n",
       "20  Intern Opportunities for Students in Germany: ...   \n",
       "21  Naturwissenschaftler - Data Analysis, Qualität...   \n",
       "22                                    Quality Analyst   \n",
       "23                             Data Scientist (m/f/x)   \n",
       "24               Data Analyst / Data Engineer (m/f/d)   \n",
       "\n",
       "                          Company                                     Location  \n",
       "0                KPMG Deutschland                     Dresden, Saxony, Germany  \n",
       "1                    datalytix.io        Stuttgart, Baden-Württemberg, Germany  \n",
       "2    Human Capital Advisory Group                             Stuttgart Region  \n",
       "3                   Audible, Inc.                      Berlin, Berlin, Germany  \n",
       "4                           Bayer                   Weimar, Thuringia, Germany  \n",
       "5                          Henkel  Düsseldorf, North Rhine-Westphalia, Germany  \n",
       "6                 Chemovator GmbH         Mannheim, Baden-Württemberg, Germany  \n",
       "7                         Pandata                      Berlin, Berlin, Germany  \n",
       "8                        audibene                      Berlin, Berlin, Germany  \n",
       "9                  Trade Republic                      Berlin, Berlin, Germany  \n",
       "10                      Sartorius              Ulm, Baden-Württemberg, Germany  \n",
       "11                    Campusjäger        Stuttgart, Baden-Württemberg, Germany  \n",
       "12                     OneMagnify     Cologne, North Rhine-Westphalia, Germany  \n",
       "13             TheSoul Publishing                                      Germany  \n",
       "14      Porsche Consulting Brasil             Tamm, Baden-Württemberg, Germany  \n",
       "15                         AMBOSS                      Berlin, Berlin, Germany  \n",
       "16                          Xcede     Cologne, North Rhine-Westphalia, Germany  \n",
       "17                        hundred                      Berlin, Berlin, Germany  \n",
       "18                   datalytix.io        Stuttgart, Baden-Württemberg, Germany  \n",
       "19  Tent Partnership for Refugees            Frankfurt am Main, Hesse, Germany  \n",
       "20                      Microsoft                     Munich, Bavaria, Germany  \n",
       "21                      BioSpring                    Frankfurt, Hesse, Germany  \n",
       "22                         TikTok                              Berlin, Germany  \n",
       "23                          CLARK            Frankfurt am Main, Hesse, Germany  \n",
       "24         uptodate Ventures GmbH                     Munich, Bavaria, Germany  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "def scrape_linkedin_job_search(keywords, location, num_page ):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ([''.join([BASE_URL, 'keywords=', keywords, \"&location=\", location, \"&start=\", str( 25 *i)])\n",
    "                   for i in range (num_page)])\n",
    "    soup = []\n",
    "    \n",
    "    for url in scrape_url:\n",
    "        if requests.get(url).status_code == 200:\n",
    "            soup.append(BeautifulSoup(requests.get(url).text, 'html.parser'))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Create a request to get the data from the server \n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for p in soup:\n",
    "        for card in p.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data\n",
    "\n",
    "results3 = scrape_linkedin_job_search('data%20analysis', 'Germany', 1)\n",
    "results3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intern Commercial Planning Intern (M/F/D)</td>\n",
       "      <td>Zalando SE</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote Data Entry Clerk - Work at Home</td>\n",
       "      <td>WW-Recruit</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Managing Director</td>\n",
       "      <td>ACL Partners</td>\n",
       "      <td>Frankfurt, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Director Business Innovation Consulting (all g...</td>\n",
       "      <td>Zühlke Group</td>\n",
       "      <td>Stuttgart Region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country Manager</td>\n",
       "      <td>Charlton Morris</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expansion Projects Manager (m/f/d)</td>\n",
       "      <td>TIER Mobility</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Global Capital Markets - Head of Private Side ...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>Frankfurt Rhine-Main Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Marketing Analyst (m/f/d)</td>\n",
       "      <td>FREE NOW (formerly mytaxi)</td>\n",
       "      <td>Hamburg, Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Internships</td>\n",
       "      <td>WWP</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Junior Projekt Manager Luftfahrt (m/w/d)</td>\n",
       "      <td>AIP SERVICES GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Strategy Consultant</td>\n",
       "      <td>LIGANOVA . The BrandRetail Company</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Planning &amp; Control Specialist, Generali Vitality</td>\n",
       "      <td>Generali</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Financial Controller</td>\n",
       "      <td>expand executive search</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Internship in Data Analytics</td>\n",
       "      <td>Henkel</td>\n",
       "      <td>Düsseldorf, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Account Manager (Food sector)</td>\n",
       "      <td>Catenon</td>\n",
       "      <td>Greater Leipzig Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Head Of Marketing</td>\n",
       "      <td>AZUM system</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Entwicklungsingenieur</td>\n",
       "      <td>SONEM Solutions GmbH</td>\n",
       "      <td>Ziemetshausen, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Head of Online Marketing</td>\n",
       "      <td>SumUp</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Client Service Associate</td>\n",
       "      <td>Finles Capital</td>\n",
       "      <td>Frankfurt Rhine-Main Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>IT Technical Support Analyst (all genders)</td>\n",
       "      <td>Watson Farley &amp; Williams</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Marketing Intern - Stuttgart</td>\n",
       "      <td>PeopleDoc Inc.</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Market Development Specialist Germany Diagnost...</td>\n",
       "      <td>Medtronic</td>\n",
       "      <td>Meerbusch, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Managing Director, Strategic Accounts - Financ...</td>\n",
       "      <td>Iron Mountain</td>\n",
       "      <td>Frankfurt Rhine-Main Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Communication Manager (m/w/d)</td>\n",
       "      <td>Unternehmensberatung GmbH</td>\n",
       "      <td>Stuttgart, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sales Executive (m/w/d)</td>\n",
       "      <td>Swiss Post Solutions</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0           Intern Commercial Planning Intern (M/F/D)   \n",
       "1              Remote Data Entry Clerk - Work at Home   \n",
       "2                                   Managing Director   \n",
       "3   Director Business Innovation Consulting (all g...   \n",
       "4                                     Country Manager   \n",
       "5                  Expansion Projects Manager (m/f/d)   \n",
       "6   Global Capital Markets - Head of Private Side ...   \n",
       "7                           Marketing Analyst (m/f/d)   \n",
       "8                                         Internships   \n",
       "9            Junior Projekt Manager Luftfahrt (m/w/d)   \n",
       "10                                Strategy Consultant   \n",
       "11   Planning & Control Specialist, Generali Vitality   \n",
       "12                               Financial Controller   \n",
       "13                       Internship in Data Analytics   \n",
       "14                      Account Manager (Food sector)   \n",
       "15                                  Head Of Marketing   \n",
       "16                              Entwicklungsingenieur   \n",
       "17                           Head of Online Marketing   \n",
       "18                           Client Service Associate   \n",
       "19         IT Technical Support Analyst (all genders)   \n",
       "20                       Marketing Intern - Stuttgart   \n",
       "21  Market Development Specialist Germany Diagnost...   \n",
       "22  Managing Director, Strategic Accounts - Financ...   \n",
       "23                      Communication Manager (m/w/d)   \n",
       "24                            Sales Executive (m/w/d)   \n",
       "\n",
       "                               Company  \\\n",
       "0                           Zalando SE   \n",
       "1                           WW-Recruit   \n",
       "2                         ACL Partners   \n",
       "3                         Zühlke Group   \n",
       "4                      Charlton Morris   \n",
       "5                        TIER Mobility   \n",
       "6                       Morgan Stanley   \n",
       "7           FREE NOW (formerly mytaxi)   \n",
       "8                                  WWP   \n",
       "9                    AIP SERVICES GmbH   \n",
       "10  LIGANOVA . The BrandRetail Company   \n",
       "11                            Generali   \n",
       "12             expand executive search   \n",
       "13                              Henkel   \n",
       "14                             Catenon   \n",
       "15                         AZUM system   \n",
       "16                SONEM Solutions GmbH   \n",
       "17                               SumUp   \n",
       "18                      Finles Capital   \n",
       "19            Watson Farley & Williams   \n",
       "20                      PeopleDoc Inc.   \n",
       "21                           Medtronic   \n",
       "22                       Iron Mountain   \n",
       "23           Unternehmensberatung GmbH   \n",
       "24                Swiss Post Solutions   \n",
       "\n",
       "                                       Location  \n",
       "0                       Berlin, Berlin, Germany  \n",
       "1                              Hamburg, Germany  \n",
       "2                     Frankfurt, Hesse, Germany  \n",
       "3                              Stuttgart Region  \n",
       "4                      Munich, Bavaria, Germany  \n",
       "5                       Berlin, Berlin, Germany  \n",
       "6        Frankfurt Rhine-Main Metropolitan Area  \n",
       "7                     Hamburg, Hamburg, Germany  \n",
       "8                       Berlin, Berlin, Germany  \n",
       "9                      Munich, Bavaria, Germany  \n",
       "10        Stuttgart, Baden-Württemberg, Germany  \n",
       "11                     Munich, Bavaria, Germany  \n",
       "12                     Munich, Bavaria, Germany  \n",
       "13  Düsseldorf, North Rhine-Westphalia, Germany  \n",
       "14                         Greater Leipzig Area  \n",
       "15                                      Germany  \n",
       "16              Ziemetshausen, Bavaria, Germany  \n",
       "17                      Berlin, Berlin, Germany  \n",
       "18       Frankfurt Rhine-Main Metropolitan Area  \n",
       "19                             Hamburg, Germany  \n",
       "20        Stuttgart, Baden-Württemberg, Germany  \n",
       "21   Meerbusch, North Rhine-Westphalia, Germany  \n",
       "22       Frankfurt Rhine-Main Metropolitan Area  \n",
       "23        Stuttgart, Baden-Württemberg, Germany  \n",
       "24                                      Germany  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search(keywords, location, num_page, num_days ):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ([''.join([BASE_URL, \"f_TPR=r\", str(num_days * 86400), 'keywords=', keywords, \"&location=\", location, \"&start=\", str( 25 *i)])\n",
    "                   for i in range (num_page)])\n",
    "    soup = []\n",
    "    \n",
    "    for url in scrape_url:\n",
    "        if requests.get(url).status_code == 200:\n",
    "            soup.append(BeautifulSoup(requests.get(url).text, 'html.parser'))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Create a request to get the data from the server \n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for p in soup:\n",
    "        for card in p.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data\n",
    "\n",
    "results4 = scrape_linkedin_job_search('data%20analysis', 'Germany', 1, 3)\n",
    "results4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search(keywords, location, num_page, num_days ):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "   currentJobId=2006865666 \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ([''.join([BASE_URL, \"f_TPR=r\", str(num_days * 86400), 'keywords=', keywords, \"&location=\", location, \"&start=\", str( 25 *i)])\n",
    "                   for i in range (num_page)])\n",
    "    soup = []\n",
    "    \n",
    "    for url in scrape_url:\n",
    "        if requests.get(url).status_code == 200:\n",
    "            soup.append(BeautifulSoup(requests.get(url).text, 'html.parser'))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Create a request to get the data from the server \n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for p in soup:\n",
    "        for card in p.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            for item in card:\n",
    "                seniority_level = card.findChild(\"h3\", )\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data\n",
    "\n",
    "results5 = scrape_linkedin_job_search('data%20analysis', 'Germany', 1, 3, )\n",
    "results5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
